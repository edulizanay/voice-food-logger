# Voice Food Logger - Development Context & Plan

## Project Overview
**Goal**: Build a proof-of-concept voice-based food logging system to validate the concept before mobile app development.

**Long-term Vision**: iPhone app where users naturally speak food intake ("I ate 150 grams of chicken and half a cup of rice") and automatically log structured data with macros/calories.

**Current Prototype**: Python Flask web app for local testing - disposable prototype focused on testing the core pipeline.

## Technical Requirements

### Core Pipeline
Audio recording â†’ Whisper transcription (Groq) â†’ LLM processing (Groq) â†’ JSON storage

### Tech Stack
- Python Flask web app
- Groq API for Whisper transcription and LLM processing
- Daily JSON file storage
- Single HTML page with embedded CSS/JS
- Local development environment

### Data Structure
Daily JSON files: `logs_YYYY-MM-DD.json`
```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "items": [
    {"food": "chicken", "quantity": "150 grams"},
    {"food": "rice", "quantity": "half cup"}
  ]
}
```

### Project Structure
```
voice-food-logger/
â”œâ”€â”€ .env                    # GROQ_API_KEY=your_key_here
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ CLAUDE.md               # This file
â”œâ”€â”€ app.py                  # Main Flask application
â”œâ”€â”€ transcription.py        # Groq Whisper API integration
â”œâ”€â”€ processing.py           # LLM food parsing logic
â”œâ”€â”€ storage.py              # JSON file management
â”œâ”€â”€ processing/
â”‚   â””â”€â”€ prompts/
â”‚       â””â”€â”€ parser.yaml     # LLM prompts for food parsing
â”œâ”€â”€ logs/                   # Daily JSON files storage
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ test_data/
â”‚   â””â”€â”€ sample_food_recording.wav    # Test audio file
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html          # All-in-one: HTML + CSS + JavaScript
â””â”€â”€ tests/
    â”œâ”€â”€ test_transcription.py
    â”œâ”€â”€ test_processing.py
    â”œâ”€â”€ test_storage.py
    â””â”€â”€ test_integration.py
```

## Development Plan & Todo List

### âœ… Completed
- [x] Create GitHub repo and initialize project structure
- [x] Set up configuration files (.env, requirements.txt, .gitignore)
- [x] Create CLAUDE.md with detailed development plan
- [x] Build transcription.py with Groq Whisper integration
- [x] Build processing.py with LLM food parsing
- [x] Build storage.py for JSON file management
- [x] Create Flask app.py with audio upload endpoint

### ðŸš§ In Progress
- [ ] Create single HTML template with embedded CSS/JS
- [ ] Create simple test files for verification
- [ ] Test complete pipeline with sample audio

### User Input Required At:
1. âœ… **DONE**: Add your GROQ API key to .env file
2. âœ… **READY**: Provide test audio recording (WAV format, food description) 
3. **NEXT**: Test web interface and provide feedback

### Key Features
- Record button with visual feedback (ready/recording/processing states)
- Real-time processing status updates
- Parsed results display
- Error handling with user-friendly messages
- Modular code structure for maintainability

### Testing Strategy
- Simple test files for each module verification
- Integration test for complete pipeline
- Sample audio file for consistent testing
- Basic error handling verification

## Notes
This is a disposable prototype focused on validating the voice â†’ structured food data pipeline. Priority is functionality over polish.



## Examples of implementation of GROQ calls that may be useful:


import os
from groq import Groq

client = Groq()
filename = os.path.dirname(__file__) + "/audio.m4a"

with open(filename, "rb") as file:
    transcription = client.audio.transcriptions.create(
      file=(filename, file.read()),
      model="whisper-large-v3-turbo",
      response_format="verbose_json",
    )
    print(transcription.text)
      


from groq import Groq

client = Groq()
completion = client.chat.completions.create(
    model="qwen/qwen3-32b",
    messages=[
      {
        "role": "user",
        "content": ""
      }
    ],
    temperature=0.6,
    max_completion_tokens=4096,
    top_p=0.95,
    reasoning_effort="default",
    stream=True,
    stop=None
)

for chunk in completion:
    print(chunk.choices[0].delta.content or "", end="")
